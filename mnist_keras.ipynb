{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import input_data\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "K.set_image_dim_ordering('th')\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Set parameters used for run\n",
    "zfac = 0.5 # The portion of points to mark as unknown\n",
    "epochs = 10\n",
    "samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from sklearn.decomposition import PCA\n",
    "#from copy import deepcopy\n",
    "\n",
    "epsilon = 0.001\n",
    "\n",
    "class ImageSet():\n",
    "    \"\"\" This a set of images. A method can dirty them up.\n",
    "        A method can recover them from a pca model\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X #The raw images\n",
    "        self.X_project = np.copy(X) #The projections of the images onto eigenspaces\n",
    "        self.y = y #The labels of the images\n",
    "        self.mean = None #The mean of the images\n",
    "        self.pca = None #The pca object of the image (Defined in scikit.PCA)\n",
    "        self.image_mask = [] #The maskes of the images (1 where image defined, 0 where not defined)\n",
    "        \n",
    "    def dirty(self, zerofac):\n",
    "        \"\"\" This method dirty's the images up by a factor of zerofac.\n",
    "            If zerofac is 1 the entire image is set to zero. If it is\n",
    "            0 the image is not changed at all.\"\"\"\n",
    "        points = np.shape(self.X)[1]\n",
    "        for iimage in range(np.shape(self.X)[0]):\n",
    "            raw_mask = np.random.ranf(points) + (0.5 - zerofac)\n",
    "            mask = np.rint(raw_mask)\n",
    "            self.X[iimage] = self.X[iimage] * mask\n",
    "            \"\"\" This last modification is a tweak to note where points were set\n",
    "                to zero for future recovery\"\"\"\n",
    "            self.X[iimage] = self.X[iimage] + epsilon*(mask-1.0)\n",
    "            image_mask = self.X[iimage] > -epsilon/2.0\n",
    "            self.image_mask.append(image_mask)\n",
    "        self.X_project = np.copy(self.X)\n",
    "        \n",
    "    def pca_calc(self, components):\n",
    "        \"\"\" This method calculates the pca model using\n",
    "            the images in this Image_set. Components is the number of components to keep \"\"\"\n",
    "\n",
    "        self.pca = PCA(n_components=components)\n",
    "        self.pca.fit(self.X_project)\n",
    "        \n",
    "    def mean_calc(self):\n",
    "        \"\"\" This method calculates the mean of the images over all images.\n",
    "            It only calculates over the clean images so masks must be \n",
    "            calculated for each image.\"\"\"\n",
    "            \n",
    "        image_count = np.zeros(np.shape(self.X)[1])\n",
    "        image_sum = np.zeros(np.shape(self.X)[1])\n",
    "        \n",
    "        for iimage in range(np.shape(self.X)[0]):\n",
    "            image = self.X[iimage]\n",
    "            image_mask = self.image_mask[iimage]\n",
    "            image_count[image_mask] += 1\n",
    "            image_sum[image_mask] += image[image_mask]\n",
    "            \n",
    "        self.mean = image_sum/image_count\n",
    "        \n",
    "    def recover_from_pca(self, pca, keep_orig = False):\n",
    "        \"\"\" This method recovers images from a passed pca object.\n",
    "            It puts the recovered image in X_project.\n",
    "            keep_orig means to keep values at original points if\n",
    "            true. If False overwrite with values from pca projection\"\"\"\n",
    "        for iimage in range(np.shape(self.X)[0]):\n",
    "            image = self.X[iimage]\n",
    "            image_prime = image - pca.mean_\n",
    "            image_mask = np.invert(self.image_mask[iimage])\n",
    "            eigen_vec = deepcopy(pca.components_)\n",
    "            for i in range(pca.n_components_):\n",
    "                eigen_vec[i][image_mask] = 0.0\n",
    "            eigen_vec_transpose = eigen_vec.transpose()\n",
    "            # Composes the matricies A and b and solves for the coefficients in A*coeff=b\n",
    "            A = eigen_vec.dot(eigen_vec_transpose)\n",
    "            b = np.zeros(pca.n_components_)\n",
    "            for i in range(pca.n_components_):\n",
    "                b[i] = image_prime.dot(eigen_vec[i])\n",
    "            coeff = np.linalg.solve(A,b)\n",
    "            \n",
    "            # Constructs the image X_project as sum of coeff*eigenvector then adds mean\n",
    "            image = np.zeros(np.shape(self.X)[1])\n",
    "            for i in range(pca.n_components_):\n",
    "                image += coeff[i] * pca.components_[i]\n",
    "            image += pca.mean_\n",
    "            \n",
    "            if keep_orig:\n",
    "                image[~image_mask] = self.X[iimage][~image_mask]\n",
    "            \n",
    "            self.X_project[iimage] = image\n",
    "            \n",
    "    def recover_from_pca_mean(self,pca):\n",
    "        \"\"\" This method replaces missing values with values from the mean computed\n",
    "           from a principle component analysis of clean images\"\"\"\n",
    "        for iimage in range(np.shape(self.X)[0]):\n",
    "            image = np.copy(self.X[iimage])\n",
    "            image_mask = np.invert(self.image_mask[iimage])\n",
    "            image[image_mask] = pca.mean_[image_mask]\n",
    "            \n",
    "            self.X_project[iimage] = image\n",
    "            \n",
    "    def recover_from_self_mean(self):\n",
    "        \" This method replaces missing values with values from own mean\"\"\"\n",
    "        \n",
    "        for iimage in range(np.shape(self.X)[0]):\n",
    "            image = np.copy(self.X[iimage])\n",
    "            image_mask = np.invert(self.image_mask[iimage])\n",
    "            image[image_mask] = self.mean[image_mask]\n",
    "            self.X_project[iimage] = image\n",
    "            \n",
    "            \n",
    "    def recover_from_self_pca(self, components, iterations, keep_orig = False):\n",
    "        \"\"\" This method recovers images from it's own image set using\n",
    "            iterative pca technique described in Everson and Sirovich\"\"\"\n",
    "        \n",
    "        # Initually fill in missing values from values in self mean\n",
    "        self.recover_from_self_mean()\n",
    "        \n",
    "        for _ in range(iterations-1):\n",
    "            print(\"doing iteration\")\n",
    "            # in each iteration calculate a pca from existing values then recover from this pca\n",
    "            self.pca_calc(components = components)\n",
    "            self.recover_from_pca(self.pca, keep_orig=False)\n",
    "\n",
    "        # In final iteration do the same thing except keep original values in place if required\n",
    "        self.pca_calc(components = components)\n",
    "        self.recover_from_pca(self.pca, keep_orig=keep_orig)  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    \"\"\" Compose the Keras model for the Convolutional Neural Network and return it\"\"\"\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "     \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(restore = None, zerofac = 0.0, model = None, components = 0, \n",
    "                keep = False, iterations = 3):\n",
    "    \"\"\" The run_example runs one example and returns the error. \n",
    "        The user passes the restore parameter of \n",
    "        \"None\", data is defined and no restoration needed\n",
    "        \"pca_mean\" restore from the mean in pca calculated from clean images\n",
    "        \"self_mean\" restore from mean of dirty images\n",
    "        \"pca\" restore from pca calculated from clean images\n",
    "        \"self_pca\" restore from pca iteratively calculated from dirty images\n",
    "        The main program then does statistics on the errors\"\"\"\n",
    "        \n",
    "#   This block of code is used to get the images off the web using input_data\n",
    "#   It writes the data to files to be read in next block as an alternative\n",
    "    data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    X_train_d, y_train_d = data.train.next_batch(50000)\n",
    "    X_test_d, y_test_d = data.test.next_batch(11000)\n",
    "    np.save(\"out_train_X\", X_train_d)\n",
    "    np.save(\"out_train_y\", y_train_d)\n",
    "    np.save(\"out_test_X\", X_test_d)\n",
    "    np.save(\"out_test_y\", y_test_d) \n",
    "\n",
    "#   The block of code reads files off disk. It can only be called when files have been\n",
    "#   written to disk in previous block. Either this block or previous block should be \n",
    "#   commented out\n",
    "    \"\"\"X_train_d = np.load(\"out_train_X.npy\")\n",
    "    y_train_d = np.load(\"out_train_y.npy\")\n",
    "    X_test_d = np.load(\"out_test_X.npy\")\n",
    "    y_test_d = np.load(\"out_test_y.npy\")\"\"\"\n",
    "\n",
    "#   Put asside a set of clean images to calculate a pca\n",
    "    N_clean = 5000\n",
    "    N_images = X_train_d.shape[0]\n",
    "    clean_images = ImageSet(np.copy(X_train_d[0:N_clean]), np.copy(y_train_d[0:N_clean]))\n",
    "    clean_images.pca_calc(components)\n",
    "\n",
    "#   Put remaining training images into the dirty_train and dirty_test Images sets\n",
    "    dirty_train = ImageSet(np.copy(X_train_d[N_clean:N_images]), np.copy(y_train_d[N_clean:N_images]))\n",
    "    dirty_test = ImageSet(np.copy(X_test_d), np.copy(y_test_d))\n",
    "    \n",
    "# Restore the images by proper method\n",
    "    if restore == None:\n",
    "        pass\n",
    "    else:\n",
    "        # Dirty up the images and calculate mean\n",
    "        dirty_train.dirty(zerofac)\n",
    "        dirty_train.mean_calc()\n",
    "        dirty_test.dirty(zerofac)\n",
    "        dirty_test.mean_calc()\n",
    "    if restore == 'pca_mean':\n",
    "        dirty_train.recover_from_pca_mean(clean_images.pca)\n",
    "        dirty_test.recover_from_pca_mean(clean_images.pca)\n",
    "    elif restore == 'self_mean':\n",
    "        dirty_train.recover_from_self_mean()\n",
    "        dirty_test.recover_from_self_mean()\n",
    "    elif restore == 'pca':\n",
    "        dirty_train.recover_from_pca(clean_images.pca, keep_orig=keep)\n",
    "        dirty_test.recover_from_pca(clean_images.pca, keep_orig=keep)\n",
    "    elif restore == 'self_pca':\n",
    "        dirty_train.recover_from_self_pca(components = components, iterations=iterations, keep_orig=True)\n",
    "        dirty_train.pca_calc(components)\n",
    "        dirty_test.mean_calc()\n",
    "        dirty_train.pca.mean_ = dirty_test.mean\n",
    "        dirty_test.recover_from_pca(dirty_train.pca, keep_orig=True)\n",
    "    elif restore == 'compress':\n",
    "        pca = PCA(n_components=components)\n",
    "        pca.fit(clean_images.X)\n",
    "        coeffs = pca.transform(dirty_train.X_project)\n",
    "        dirty_train.X_project = pca.inverse_transform(coeffs)\n",
    "        coeffs = pca.transform(dirty_test.X_project)\n",
    "        dirty_test.X_project = pca.inverse_transform(coeffs)\n",
    "        diff = np.sum(np.absolute(dirty_train.X - dirty_train.X_project))\n",
    "        mag = np.sum(np.absolute(dirty_train.X))\n",
    "        print(\"difference is \", diff, mag, \"with components\", components)\n",
    "\n",
    "    X_dirty_train = dirty_train.X_project\n",
    "    X_dirty_test = dirty_test.X_project\n",
    "\n",
    "    X_train = X_dirty_train.reshape(X_dirty_train.shape[0], 1, 28, 28).astype('float32')\n",
    "    X_test = X_dirty_test.reshape(X_dirty_test.shape[0], 1, 28, 28).astype('float32')\n",
    "    \n",
    "# normalize inputs from 0-255 to 0-1\n",
    "    X_train = X_train / 255\n",
    "    X_test = X_test / 255\n",
    "\n",
    "    y_train = dirty_train.y\n",
    "    y_test = dirty_test.y\n",
    "\n",
    "# build the CNN model (see larger_model function)\n",
    "    model = larger_model()\n",
    "# Fit the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=100, verbose=1)\n",
    "# Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "    \n",
    "    if K.backend() == 'tensorflow':\n",
    "        K.clear_session()\n",
    "    del dirty_train\n",
    "    del dirty_test\n",
    "    del clean_images\n",
    "    \n",
    "    return (100-scores[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing clean on sample  0\n",
      "WARNING:tensorflow:From <ipython-input-6-98f3c6400150>:14: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Train on 45000 samples, validate on 11000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 80s 2ms/step - loss: 1.7237 - acc: 0.5143 - val_loss: 0.7954 - val_acc: 0.7929\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 79s 2ms/step - loss: 0.6638 - acc: 0.8001 - val_loss: 0.4998 - val_acc: 0.8528\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 79s 2ms/step - loss: 0.5011 - acc: 0.8465 - val_loss: 0.4023 - val_acc: 0.8816\n",
      "Epoch 4/10\n",
      "28700/45000 [==================>...........] - ETA: 25s - loss: 0.4274 - acc: 0.8686"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fc583d88ac64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doing clean on sample \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzerofac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzfac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mclean_errors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-98f3c6400150>\u001b[0m in \u001b[0;36mrun_example\u001b[1;34m(restore, zerofac, model, components, keep, iterations)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlarger_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"This is the main script. It restores and fits the data with various methods using the \n",
    "    run_example method. For each restoration method it loops through samples to calculate\n",
    "    statistics. It then writes these statistics to a csv file.\"\"\"\n",
    "start = datetime.datetime.now()\n",
    "error = 100\n",
    "clean_errors = np.zeros(samples)\n",
    "for i in range(samples):\n",
    "    print(\"doing clean on sample \",i)\n",
    "    error = run_example(restore = None, zerofac = zfac)\n",
    "    clean_errors[i] = error\n",
    "\n",
    "pca_mean_errors = np.zeros(samples)\n",
    "for i in range(samples):\n",
    "    print(\"doing pca_mean on sample \",i)\n",
    "    error = run_example(restore = 'pca_mean', zerofac = zfac)\n",
    "    pca_mean_errors[i] = error\n",
    "print('the mean for pca_mean is %5.3f with standard deviation of %5.3f', \n",
    "      (np.mean(pca_mean_errors), np.std(pca_mean_errors)))\n",
    "\n",
    "    \n",
    "self_mean_errors = np.zeros(samples)\n",
    "for i in range(samples):\n",
    "    print(\"doing self_mean on sample \",i)\n",
    "    error = run_example(restore = 'self_mean', zerofac = zfac)\n",
    "    self_mean_errors[i] = error\n",
    "print('the mean for self_mean is %5.3f with standard deviation of %5.3f', \n",
    "      (np.mean(self_mean_errors), np.std(self_mean_errors)))\n",
    "    \n",
    "keep = True\n",
    "components = 100\n",
    "pca_errors = np.zeros(samples)\n",
    "for i in range(samples):\n",
    "    print(\"doing pca on sample \",i)\n",
    "    error = run_example(restore = 'pca', zerofac = zfac, components = components, keep = keep)\n",
    "    pca_errors[i] = error\n",
    "print('the mean with %s and %d components is %5.3f with standard deviation of %5.3f', (keep, components, np.mean(pca_errors), np.std(pca_errors)))\n",
    "\n",
    "keep = True\n",
    "iterations = 4\n",
    "components = 100\n",
    "self_pca_errors = np.zeros(samples)\n",
    "for i in range(samples):\n",
    "    print(\"doing self_pca on sample \",i)\n",
    "    error = run_example(restore = 'self_pca', zerofac = zfac, components = components, keep = keep, iterations=iterations)\n",
    "    self_pca_errors[i] = error\n",
    "print('the mean with %s and %d components is %5.3f with standard deviation of %5.3f', (keep, components, np.mean(self_pca_errors), np.std(self_pca_errors)))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"The job took \", end-start)\n",
    "\n",
    "\n",
    "\"\"\" Now output the result \"\"\"\n",
    "outfile = 'Result/result'\n",
    "outfile += ('_zfac'+str(zfac))\n",
    "outfile += ('_epochs'+str(epochs))\n",
    "outfile += ('_comps'+str(components))\n",
    "outfile += ('_iters'+str(iterations))\n",
    "\n",
    "#np.savez(outfile,clean_errors,pca_mean_errors,self_mean_errors, pca_errors, self_pca_errors)\n",
    "\n",
    "initial = [[None for _ in range(samples+3)] for _ in range(6)]\n",
    "col_array = [\"\" for x in range(samples+2)]\n",
    "for col in range(samples):\n",
    "    col_array[col] = 'sample '+str(col)\n",
    "col_array[samples] = 'mean'\n",
    "col_array[samples+1] = 'standard deviation'\n",
    "    \n",
    "index_array = [\"\" for x in range(5)]\n",
    "\n",
    "index_array[0] = \"clean_errors\"\n",
    "#clean_errors = np.random.randint(10,size=samples)\n",
    "for i in range(samples):\n",
    "    initial[1][i+1] = clean_errors[i]\n",
    "initial[1][samples+1] = np.mean(clean_errors)\n",
    "initial[1][samples+2] = np.std(clean_errors)\n",
    "    \n",
    "index_array[1] = \"pca_mean_errors\"\n",
    "#pca_mean_errors = np.random.randint(10,size=samples)\n",
    "for i in range(samples):\n",
    "    initial[2][i+1] = pca_mean_errors[i]\n",
    "initial[2][samples+1] = np.mean(pca_mean_errors)\n",
    "initial[2][samples+2] = np.std(pca_mean_errors)\n",
    "    \n",
    "index_array[2] = \"self_mean_errors\"\n",
    "#self_mean_errors = np.random.randint(10,size=samples)\n",
    "for i in range(samples):\n",
    "    initial[3][i+1] = self_mean_errors[i]\n",
    "initial[3][samples+1] = np.mean(self_mean_errors)\n",
    "initial[3][samples+2] = np.std(self_mean_errors)\n",
    "\n",
    "index_array[3] = \"pca_errors\"\n",
    "#pca_errors = np.random.randint(10,size=samples)\n",
    "for i in range(samples):\n",
    "    initial[4][i+1] = pca_errors[i]\n",
    "initial[4][samples+1] = np.mean(pca_errors)\n",
    "initial[4][samples+2] = np.std(pca_errors)\n",
    "    \n",
    "index_array[4] = \"self_pca_errors\"\n",
    "#self_pca_errors = np.random.randint(10,size=samples)\n",
    "for i in range(samples):\n",
    "    initial[5][i+1] = self_pca_errors[i]\n",
    "initial[5][samples+1] = np.mean(self_pca_errors)\n",
    "initial[5][samples+2] = np.std(self_pca_errors)\n",
    "\n",
    "data3_np = np.array(initial)\n",
    "\n",
    "data3_df = pd.DataFrame(data=data3_np[1:,1:],\n",
    "                        index=np.array(index_array),\n",
    "                        columns=np.array(col_array))\n",
    "outfile += '.csv'\n",
    "\n",
    "data3_df.to_csv(outfile, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
